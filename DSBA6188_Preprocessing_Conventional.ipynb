{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/realbluesnail/UNCC_DSBA6188/blob/main/DSBA6188_Preprocessing_Conventional.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup\n",
        "- Import packages\n",
        "- [Optional] Mount your google drive (if you would like to you read and save data files from your google drive)"
      ],
      "metadata": {
        "id": "BX3rqwFJfIGk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0HyZE2mYwPX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "# from wordcloud import WordCloud ## don't need it this time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import nltk.corpus\n"
      ],
      "metadata": {
        "id": "8GahiJedZF3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing word_tokenize from nltk\n",
        "from nltk.tokenize import (word_tokenize,\n",
        "                           sent_tokenize,\n",
        "                           TreebankWordTokenizer,\n",
        "                           wordpunct_tokenize,\n",
        "                           TweetTokenizer,\n",
        "                           MWETokenizer)\n",
        "# Get the tokenizer to divide text into sentences\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QxNjtILLKcG",
        "outputId": "aabe2766-698b-42a7-df3a-54096b02f267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you would like to save and read data files from your Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "eN0fbUHcOPWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e86ffe-e97f-4718-acf5-1022b01e5ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "\n",
        "- Expriment and compare different tokenization techniques\n"
      ],
      "metadata": {
        "id": "Cl5nDm6Vfx8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text: a twitter message\n",
        "\n",
        "sample_text = \"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer. Terrible! You shoulda got David Carr Carr of Third Day to do it. ;D ‚òπÔ∏èüëΩ\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "te8gorUHTZM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Whitespace tokenization\n",
        "tokens = sample_text.split(',')\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "K1TkhaNScxPV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57a43c36-15da-4ed5-f709-f4f618007a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@switchfoot http://twitpic.com/2y1zl - Awww', \" that's a bummer. Terrible! You shoulda got David Carr of Third Day to do it. ;D ‚òπÔ∏èüëΩ\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split text using a specfic symbol (or letter)\n",
        "tokens = sample_text.split('w')\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "W10KaEjydWzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a634db-3da9-4250-a80f-85dabf89e25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@s', 'itchfoot http://t', 'itpic.com/2y1zl - A', '', '', \", that's a bummer. Terrible! You shoulda got David Carr of Third Day to do it. ;D ‚òπÔ∏èüëΩ\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different tokenization tools under NLTK\n",
        "\n",
        "# 1. Word tokenizer\n",
        "# from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(sample_text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "1Fsw9TvPzWjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "328af30e-8012-4751-cff3-290ad586f53a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@', 'switchfoot', 'http', ':', '//twitpic.com/2y1zl', '-', 'Awww', ',', 'that', \"'s\", 'a', 'bummer', '.', 'Terrible', '!', 'You', 'shoulda', 'got', 'David', 'Carr', 'of', 'Third', 'Day', 'to', 'do', 'it', '.', ';', 'D', '‚òπÔ∏èüëΩ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Word punctuation tokenizer\n",
        "# from nltk.tokenize import wordpunct_tokenize\n",
        "tokens = wordpunct_tokenize(sample_text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "dmaeGhnhdqEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "affdae10-143d-49c6-d7c9-7dd3c5774d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@', 'switchfoot', 'http', '://', 'twitpic', '.', 'com', '/', '2y1zl', '-', 'Awww', ',', 'that', \"'\", 's', 'a', 'bummer', '.', 'Terrible', '!', 'You', 'shoulda', 'got', 'David', 'Carr', 'of', 'Third', 'Day', 'to', 'do', 'it', '.', ';', 'D', '‚òπÔ∏èüëΩ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Treebank word tokenizer\n",
        "# from nltk.tokenize import TreebankWordTokenizer\n",
        "my_tokenizer = TreebankWordTokenizer()\n",
        "tokens = my_tokenizer.tokenize(sample_text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "tOMYjhjVLBCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0643d5aa-d476-486c-99c4-2885c1fddb0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@', 'switchfoot', 'http', ':', '//twitpic.com/2y1zl', '-', 'Awww', ',', 'that', \"'s\", 'a', 'bummer.', 'Terrible', '!', 'You', 'shoulda', 'got', 'David', 'Carr', 'of', 'Third', 'Day', 'to', 'do', 'it.', ';', 'D', '‚òπÔ∏èüëΩ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Tweet Tokenizer\n",
        "# from nltk.tokenize import TweetTokenizer\n",
        "my_tokenizer = TweetTokenizer()\n",
        "tokens = my_tokenizer.tokenize(sample_text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0eHEvJ9E9YJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c6a46de-a583-4dab-f3b6-56847ce7ccd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@switchfoot', 'http://twitpic.com/2y1zl', '-', 'Awww', ',', \"that's\", 'a', 'bummer', '.', 'Terrible', '!', 'You', 'shoulda', 'got', 'David', 'Carr', 'of', 'Third', 'Day', 'to', 'do', 'it', '.', ';D', '‚òπ', 'Ô∏è', 'üëΩ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. MWE Tokenizer\n",
        "# from nltk.tokenize import MWETokenizer\n",
        "my_tokenizer = MWETokenizer()\n",
        "my_tokenizer.add_mwe(('David', 'Carr'))\n",
        "tokens = my_tokenizer.tokenize(TweetTokenizer().tokenize(sample_text))\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "hXyNOlc0LNBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df98a96a-d11e-40ea-dbba-c6f6590da8e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@switchfoot', 'http://twitpic.com/2y1zl', '-', 'Awww', ',', \"that's\", 'a', 'bummer', '.', 'Terrible', '!', 'You', 'shoulda', 'got', 'David_Carr', 'of', 'Third', 'Day', 'to', 'do', 'it', '.', ';D', '‚òπ', 'Ô∏è', 'üëΩ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SpaCy\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n"
      ],
      "metadata": {
        "id": "_XxjE2-QK9n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sample_text)\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "45u2yqNXMYM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c23688-dad6-4571-915d-0bfc80e582d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@switchfoot', 'http://twitpic.com/2y1zl', '-', 'Awww', ',', 'that', \"'s\", 'a', 'bummer', '.', 'Terrible', '!', 'You', 'shoulda', 'got', 'David', 'Carr', 'of', 'Third', 'Day', 'to', 'do', 'it', '.', ';D', '‚òπ', 'Ô∏è', 'üëΩ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gensim\n",
        "from gensim.utils import tokenize\n",
        "tokens = list(tokenize(sample_text))\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "Ss9ya7zvNS4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "521c7b01-89aa-4fb4-d9b4-0fa7a56956f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['switchfoot', 'http', 'twitpic', 'com', 'y', 'zl', 'Awww', 'that', 's', 'a', 'bummer', 'Terrible', 'You', 'shoulda', 'got', 'David', 'Carr', 'of', 'Third', 'Day', 'to', 'do', 'it', 'D']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "tokens = text_to_word_sequence(sample_text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "tTTS5PjMNwKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c406c0f0-2182-43a8-97a1-3e7abe179d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['switchfoot', 'http', 'twitpic', 'com', '2y1zl', 'awww', \"that's\", 'a', 'bummer', 'terrible', 'you', 'shoulda', 'got', 'david', 'carr', 'of', 'third', 'day', 'to', 'do', 'it', 'd', '‚òπÔ∏èüëΩ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "my_tokenizer = Tokenizer()\n",
        "my_tokenizer.fit_on_texts([sample_text])\n",
        "tokens = my_tokenizer.texts_to_sequences([sample_text])\n",
        "print(my_tokenizer.word_index)\n"
      ],
      "metadata": {
        "id": "ZlbaGD3WQI5k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e33e992-d1ab-4cec-e67d-f997a4122ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'carr': 1, 'switchfoot': 2, 'http': 3, 'twitpic': 4, 'com': 5, '2y1zl': 6, 'awww': 7, \"that's\": 8, 'a': 9, 'bummer': 10, 'terrible': 11, 'you': 12, 'shoulda': 13, 'got': 14, 'david': 15, 'of': 16, 'third': 17, 'day': 18, 'to': 19, 'do': 20, 'it': 21, 'd': 22, '‚òπÔ∏èüëΩ': 23}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Other tokenizers\n",
        "# Sentence tokenizer\n",
        "tokens = sent_tokenize(sample_text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "cUqvq9DiKuYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7047576-1005-4b7c-a7de-1f9999d73ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.\", 'Terrible!', 'You shoulda got David Carr Carr of Third Day to do it.', ';D ‚òπÔ∏èüëΩ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subwords tokenizer\n"
      ],
      "metadata": {
        "id": "dZTNDBybycOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing raw text\n",
        "\n",
        "- Remove Stopwords\n",
        "- Remove Punctuations\n",
        "- Stemming\n",
        "- Lemmatization\n",
        "- Remove URLs"
      ],
      "metadata": {
        "id": "v7j9rZcLScmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "tokens = word_tokenize(sample_text)\n",
        "print(tokens)\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "print(filtered_tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kGf8CkW6S6LK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcac4fbc-9971-47bb-feb5-4d457b190269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@', 'switchfoot', 'http', ':', '//twitpic.com/2y1zl', '-', 'Awww', ',', 'that', \"'s\", 'a', 'bummer', '.', 'Terrible', '!', 'You', 'shoulda', 'got', 'David', 'Carr', 'Carr', 'of', 'Third', 'Day', 'to', 'do', 'it', '.', ';', 'D', '‚òπÔ∏èüëΩ']\n",
            "['@', 'switchfoot', 'http', ':', '//twitpic.com/2y1zl', '-', 'Awww', ',', \"'s\", 'bummer', '.', 'Terrible', '!', 'shoulda', 'got', 'David', 'Carr', 'Carr', 'Third', 'Day', '.', ';', '‚òπÔ∏èüëΩ']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "tokens = word_tokenize(sample_text)\n",
        "print(tokens)\n",
        "filtered_tokens = [token for token in tokens if token not in string.punctuation]\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "id": "e_b_HVkQS_IO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6ca4055-0997-49d8-b4a6-72159632f223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@', 'switchfoot', 'http', ':', '//twitpic.com/2y1zl', '-', 'Awww', ',', 'that', \"'s\", 'a', 'bummer', '.', 'Terrible', '!', 'You', 'shoulda', 'got', 'David', 'Carr', 'Carr', 'of', 'Third', 'Day', 'to', 'do', 'it', '.', ';', 'D', '‚òπÔ∏èüëΩ']\n",
            "['switchfoot', 'http', '//twitpic.com/2y1zl', 'Awww', 'that', \"'s\", 'a', 'bummer', 'Terrible', 'You', 'shoulda', 'got', 'David', 'Carr', 'Carr', 'of', 'Third', 'Day', 'to', 'do', 'it', 'D', '‚òπÔ∏èüëΩ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "#defining the object for stemming\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "tokens = word_tokenize(sample_text)\n",
        "print(tokens)\n",
        "stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
        "print(stemmed_tokens)\n"
      ],
      "metadata": {
        "id": "EdDyUFpvjzYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082bd30b-7aa7-40ef-b40d-ac16de317c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@', 'switchfoot', 'http', ':', '//twitpic.com/2y1zl', '-', 'Awww', ',', 'that', \"'s\", 'a', 'bummer', '.', 'Terrible', '!', 'You', 'shoulda', 'got', 'David', 'Carr', 'Carr', 'of', 'Third', 'Day', 'to', 'do', 'it', '.', ';', 'D', '‚òπÔ∏èüëΩ']\n",
            "['@', 'switchfoot', 'http', ':', '//twitpic.com/2y1zl', '-', 'awww', ',', 'that', \"'s\", 'a', 'bummer', '.', 'terribl', '!', 'you', 'shoulda', 'got', 'david', 'carr', 'carr', 'of', 'third', 'day', 'to', 'do', 'it', '.', ';', 'd', '‚òπÔ∏èüëΩ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization 1\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download necessary data if you haven't already\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "tokens = word_tokenize(\"good better great goose geese meeting meet meets going go gone\")\n",
        "print(tokens)\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token, pos = 'n') for token in tokens]\n",
        "print(lemmatized_tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "0xhmXiCzqRbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a795608a-95b5-4c3c-919e-e4be9cadbfda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['good', 'better', 'great', 'goose', 'geese', 'meeting', 'meet', 'meets', 'going', 'go', 'gone']\n",
            "['good', 'better', 'great', 'goose', 'goose', 'meeting', 'meet', 'meet', 'going', 'go', 'gone']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization 2\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "#defining the object for Lemmatization\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# POS_TAGGER_FUNCTION : TYPE 1\n",
        "def pos_tagger(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "tokens = word_tokenize(\"good better great goose geese meeting meet going go gone\")\n",
        "\n",
        "tokens_pos_tagged = nltk.pos_tag(tokens)\n",
        "print(tokens_pos_tagged)\n",
        "\n",
        "tokens_wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), tokens_pos_tagged))\n",
        "print(tokens_wordnet_tagged)\n",
        "\n",
        "tokens_lemma = []\n",
        "for w, tag in tokens_wordnet_tagged:\n",
        "    if tag is None:\n",
        "        tokens_lemma.append(w)\n",
        "    else:\n",
        "        tokens_lemma.append(lemmatizer.lemmatize(w, tag))\n",
        "\n",
        "#tokens_lemma = \" \".join(tokens_lemma)\n",
        "print(tokens)\n",
        "print(tokens_lemma)"
      ],
      "metadata": {
        "id": "4e_nftpen7Z-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9fee8d6-0f7c-4a67-ea8f-f02a42448e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('good', 'JJ'), ('better', 'JJR'), ('great', 'JJ'), ('goose', 'JJ'), ('geese', 'JJ'), ('meeting', 'NN'), ('meet', 'NN'), ('going', 'VBG'), ('go', 'VB'), ('gone', 'VBN')]\n",
            "[('good', 'a'), ('better', 'a'), ('great', 'a'), ('goose', 'a'), ('geese', 'a'), ('meeting', 'n'), ('meet', 'n'), ('going', 'v'), ('go', 'v'), ('gone', 'v')]\n",
            "['good', 'better', 'great', 'goose', 'geese', 'meeting', 'meet', 'going', 'go', 'gone']\n",
            "['good', 'good', 'great', 'goose', 'geese', 'meeting', 'meet', 'go', 'go', 'go']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URL using Regular Expression\n",
        "import re # Regular Expression\n",
        "\n",
        "# Define a regular expression pattern\n",
        "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "\n",
        "# tokens = word_tokenize(sample_text)\n",
        "# print(tokens)\n",
        "# filtered_tokens = [token for token in tokens if not url_pattern.search(token)]\n",
        "# print(filtered_tokens)\n",
        "\n",
        "sample_text_wo_url = url_pattern.sub('', sample_text)\n",
        "tokens = word_tokenize(sample_text_wo_url)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "7dsQUVI7xZoU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "664dca8b-2503-45ae-d87f-915796d9dc78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@', 'switchfoot', '-', 'Awww', ',', 'that', \"'s\", 'a', 'bummer', '.', 'Terrible', '!', 'You', 'shoulda', 'got', 'David', 'Carr', 'Carr', 'of', 'Third', 'Day', 'to', 'do', 'it', '.', ';', 'D', '‚òπÔ∏èüëΩ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nbKwAT6-x0Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Take home Assignment:\n",
        "Create your own pre-processing pipeline (from raw text to tokens)"
      ],
      "metadata": {
        "id": "SsJj6ktkfo3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Optional step]: If you would like to take a look at more sample data from the twitter dataset\n",
        "# To load the example data set, not that you might need to change the file path to where you save the tweeter_training.csv. The data set is also avaiable on canvas for download\n",
        "tw_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/tweeter_training.csv', encoding='ISO-8859-1', header=None)\n",
        "column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
        "tw_df.columns = column_names"
      ],
      "metadata": {
        "id": "1evDcLfYOEsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4QAfICaRHpEi"
      }
    }
  ]
}